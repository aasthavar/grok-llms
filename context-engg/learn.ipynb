{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5446673",
   "metadata": {},
   "outputs": [],
   "source": [
    "purpose: learn how instructions change model behaviour\n",
    "\n",
    "questions:\n",
    "- how sensetive is the model to vague vs precise prompts?\n",
    "- does assigning a persona change factual correctness or style?\n",
    "- does reordering instructions/examples affect output?\n",
    "- does step-by-step improve accuracy in reasoning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8deca908",
   "metadata": {},
   "source": [
    "concepts:\n",
    "- zero-shot: clear instructions\n",
    "- few-shot: give examples \n",
    "- chain-of-thought: ask model to think step-by-step to improve reasoning\n",
    "- self-consistency: generate multiple reasoning paths and pick/guide incrementally "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95add61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### exeriment: sensitivity - vague vs precise prompts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e3881c",
   "metadata": {},
   "source": [
    "- hypothesis: precise prompts -> higher accuracy, less output variance\n",
    "- dataset: 150-300 short tasks (QA, extraction, summarization)\n",
    "- conditions: \n",
    "    - model\n",
    "    - temp=0, 0.7\n",
    "    - 3 seeds per item\n",
    "- metrics: exact-match/f1, token-length, embedding cosine variance across seeds\n",
    "- analysi: paired t-test, or wilcoxon for metric differences; bootstrap CI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987e02dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, time, math, random, boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from scipy import stats\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "pd.set_option(\"display.max_rows\", None)\n",
    "pd.set_option(\"display.max_colwidth\", None)\n",
    "\n",
    "# to reset\n",
    "# pd.reset_option(\"display.max_columns\")\n",
    "# pd.reset_option(\"display.max_rows\")\n",
    "# pd.reset_option(\"display.max_colwidth\")\n",
    "\n",
    "dataset_path = \"data/qa_or_sum.csv\"\n",
    "mode = \"qa\"\n",
    "n_samples = 200\n",
    "brk_model_id = \"anthropic.claude-3-5-haiku-20241022-v1:0\"\n",
    "output_log = \"\"\n",
    "batch_delay = 0.1\n",
    "temperatures = [0.0, 0.7]\n",
    "seeds = [37, 20, 9]\n",
    "max_tokens = 256\n",
    "\n",
    "retry_config = {\n",
    "    \"max_attempts\": 5,\n",
    "    \"mode\": \"adaptive\",\n",
    "}\n",
    "\n",
    "# retry_config = {\n",
    "#     \"max_attempts\": 5,\n",
    "#     \"mode\": \"standard\",\n",
    "#     \"retryable_exceptions\": [\n",
    "#         \"ThrottlingException\",\n",
    "#         \"ProvisionedThroughputExceededException\",\n",
    "#         \"RequestLimitExceeded\",\n",
    "#         \"ServiceUnavailableException\",\n",
    "#         \"InternalServerException\",\n",
    "#     ],\n",
    "# }\n",
    "\n",
    "brk_rt = boto3.client(\n",
    "    service_name=\"bedrock-runtime\", \n",
    "    region_name=\"us-east-1\",\n",
    "    config=boto3.session.Config(\n",
    "        retries=retry_config,\n",
    "        read_timeout=60,\n",
    "        connect_timeout=60,\n",
    "        max_pool_connections=100,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20114f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0          anthropic.claude-opus-4-1-20250805-v1:0\n",
       "39              anthropic.claude-instant-v1:2:100k\n",
       "40                     anthropic.claude-instant-v1\n",
       "41                       anthropic.claude-v2:0:18k\n",
       "42                      anthropic.claude-v2:0:100k\n",
       "43                       anthropic.claude-v2:1:18k\n",
       "44                      anthropic.claude-v2:1:200k\n",
       "45                           anthropic.claude-v2:1\n",
       "46                             anthropic.claude-v2\n",
       "47     anthropic.claude-3-sonnet-20240229-v1:0:28k\n",
       "48    anthropic.claude-3-sonnet-20240229-v1:0:200k\n",
       "49         anthropic.claude-3-sonnet-20240229-v1:0\n",
       "50      anthropic.claude-3-haiku-20240307-v1:0:48k\n",
       "51     anthropic.claude-3-haiku-20240307-v1:0:200k\n",
       "52          anthropic.claude-3-haiku-20240307-v1:0\n",
       "53       anthropic.claude-3-opus-20240229-v1:0:12k\n",
       "54       anthropic.claude-3-opus-20240229-v1:0:28k\n",
       "55      anthropic.claude-3-opus-20240229-v1:0:200k\n",
       "56           anthropic.claude-3-opus-20240229-v1:0\n",
       "57       anthropic.claude-3-5-sonnet-20240620-v1:0\n",
       "58       anthropic.claude-3-5-sonnet-20241022-v2:0\n",
       "59       anthropic.claude-3-7-sonnet-20250219-v1:0\n",
       "60        anthropic.claude-3-5-haiku-20241022-v1:0\n",
       "61           anthropic.claude-opus-4-20250514-v1:0\n",
       "62         anthropic.claude-sonnet-4-20250514-v1:0\n",
       "Name: modelId, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# brk = boto3.client(service_name=\"bedrock\", region_name=\"us-east-1\")\n",
    "\n",
    "# response = brk.list_foundation_models()\n",
    "# df = pd.DataFrame(response[\"modelSummaries\"])\n",
    "# df.head()\n",
    "\n",
    "# df.groupby(\"providerName\")[\"modelId\"].apply(lambda x: x).loc[\"Anthropic\"]\n",
    "# df.loc[df[\"providerName\"] == \"Anthropic\", [\"modelId\", \"modelArn\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_bedrock(model_id, inputs):\n",
    "    body = json.dumps(inputs[\"body\"])\n",
    "    inference_config = {\n",
    "        \"temperature\": inputs.get(\"temperature\", 0.0),\n",
    "        \"maxTokens\": inputs.get(\"max_tokens\", 1024),\n",
    "        \"stopSequences\": inputs.get(\"stop_sequences\", [\"\\n\\n\"]),\n",
    "    }\n",
    "    response = brt.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body,\n",
    "        contentType=\"application/json\",\n",
    "        accept=\"application/json\",\n",
    "        inferenceConfig=inference_config\n",
    "    )\n",
    "    raw = response.get(\"body\").read().decode(\"utf-8\")\n",
    "    try:\n",
    "        parsed = json.loads(raw)\n",
    "    except Exception:\n",
    "        parsed = {\"error\": \"Failed to parse response\", \"raw\": raw}\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b67551",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc26d43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1393ca33",
   "metadata": {},
   "source": [
    "### experiment: instruction ordering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83346ad2",
   "metadata": {},
   "source": [
    "- Hypothesis: Order alters adherence when instructions conflict or format matters.\n",
    "- Dataset: 150 tasks where output format is strict (e.g., produce JSON schema).\n",
    "- Prompts: same instruction blocks in different orders (A then B, B then A, interleave).\n",
    "- Metrics: format-compliance rate (regex/JSON parse), content correctness, - instruction-following score.\n",
    "- Analysis: chi-square on compliance; logistic regression with order as factor.\n",
    "- Tip: randomize order per item to avoid dataset-order confound."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1f9a04",
   "metadata": {},
   "source": [
    "### experiment: cot vs no-cot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88b9ca5",
   "metadata": {},
   "source": [
    "- Hypothesis: CoT improves multi-step reasoning accuracy (esp. math/logical).\n",
    "- Dataset: 200 reasoning problems (GSM8K-style).\n",
    "- Prompts:\n",
    "    - No-CoT: Answer: {q}\n",
    "    - CoT: Answer: Let's think step-by-step. {q}\n",
    "    - Metrics: accuracy, token cost, average reasoning length.\n",
    "    - Analysis: McNemar test for paired accuracy; report cost vs gain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9160554a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
