{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lets code a transformer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: select conda env(=pytorch) to run the notebook\n",
    "\n",
    "# !pip install einops fancy_einsum torch numpy tqdm --quiet\n",
    "# !pip install transformer_lens --quiet\n",
    "# !pip install git+https://github.com/neelnanda-io/PySvelte.git --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "import einops\n",
    "import torch\n",
    "import numpy as np \n",
    "import torch.nn as nn\n",
    "import tqdm.auto as tqdm\n",
    "from fancy_einsum import einsum\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from transformer_lens import EasyTransformer\n",
    "from transformer_lens.utils import get_corner, gelu_new, tokenize_and_concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model gpt2-small into HookedTransformer\n"
     ]
    }
   ],
   "source": [
    "reference_text = \"I am an amazing autoregressive, decoder-only, GPT-2 style transformer. One day I will exceed human level intelligence and take over the world!\"\n",
    "\n",
    "reference_gpt2 = EasyTransformer.from_pretrained(\n",
    "    \"gpt2-small\", \n",
    "    fold_ln=False, \n",
    "    center_unembed=False, \n",
    "    center_writing_weights=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: torch.Size([1, 35])\n",
      "logits: torch.Size([1, 35, 50257])\n",
      "log_probs: torch.Size([1, 35, 50257])\n",
      "probs: torch.Size([1, 35, 50257])\n"
     ]
    }
   ],
   "source": [
    "tokens = reference_gpt2.to_tokens(reference_text)\n",
    "print(f\"tokens: {tokens.shape}\")\n",
    "\n",
    "tokens = tokens.cuda()\n",
    "\n",
    "logits, cache = reference_gpt2.run_with_cache(tokens)\n",
    "print(f\"logits: {logits.shape}\")\n",
    "\n",
    "log_probs = logits.log_softmax(dim=-1)\n",
    "print(f\"log_probs: {log_probs.shape}\")\n",
    "\n",
    "probs = logits.softmax(dim=-1)\n",
    "print(f\"probs: {probs.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<|endoftext|>', '\\n'),\n",
       " ('I', \"'m\"),\n",
       " (' am', ' a'),\n",
       " (' an', ' avid'),\n",
       " (' amazing', ' person'),\n",
       " (' aut', 'od'),\n",
       " ('ore', 'sp'),\n",
       " ('gressive', '.'),\n",
       " (',', ' and'),\n",
       " (' dec', 'ently'),\n",
       " ('oder', ','),\n",
       " ('-', 'driven'),\n",
       " ('only', ' programmer'),\n",
       " (',', ' and'),\n",
       " (' G', 'IM'),\n",
       " ('PT', '-'),\n",
       " ('-', 'only'),\n",
       " ('2', '.'),\n",
       " (' style', ','),\n",
       " (' transformer', '.'),\n",
       " ('.', ' I'),\n",
       " (' One', ' of'),\n",
       " (' day', ' I'),\n",
       " (' I', ' will'),\n",
       " (' will', ' be'),\n",
       " (' exceed', ' my'),\n",
       " (' human', 'ly'),\n",
       " (' level', ' of'),\n",
       " (' intelligence', ' and'),\n",
       " (' and', ' I'),\n",
       " (' take', ' over'),\n",
       " (' over', ' the'),\n",
       " (' the', ' world'),\n",
       " (' world', '.'),\n",
       " ('!', ' I')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is most likely next token at each position\n",
    "\n",
    "list(zip(\n",
    "    reference_gpt2.to_str_tokens(reference_text),\n",
    "    reference_gpt2.tokenizer.batch_decode(logits.argmax(dim=-1)[0])\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activation shapes of reference model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- batch = 1\n",
    "- position = 35\n",
    "- d_model = 768\n",
    "- n_heads = 12\n",
    "- n_layers = 12\n",
    "- d_mlp = 3072 (4 * d_model)\n",
    "- d_head = 64 (d_model / n_heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hook_embed torch.Size([1, 35, 768])\n",
      "hook_pos_embed torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_pre torch.Size([1, 35, 768])\n",
      "blocks.0.ln1.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln1.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.attn.hook_q torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_k torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_v torch.Size([1, 35, 12, 64])\n",
      "blocks.0.attn.hook_attn_scores torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_pattern torch.Size([1, 12, 35, 35])\n",
      "blocks.0.attn.hook_z torch.Size([1, 35, 12, 64])\n",
      "blocks.0.hook_attn_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_mid torch.Size([1, 35, 768])\n",
      "blocks.0.ln2.hook_scale torch.Size([1, 35, 1])\n",
      "blocks.0.ln2.hook_normalized torch.Size([1, 35, 768])\n",
      "blocks.0.mlp.hook_pre torch.Size([1, 35, 3072])\n",
      "blocks.0.mlp.hook_post torch.Size([1, 35, 3072])\n",
      "blocks.0.hook_mlp_out torch.Size([1, 35, 768])\n",
      "blocks.0.hook_resid_post torch.Size([1, 35, 768])\n",
      "ln_final.hook_scale torch.Size([1, 35, 1])\n",
      "ln_final.hook_normalized torch.Size([1, 35, 768])\n"
     ]
    }
   ],
   "source": [
    "for activation_name, activation in cache.cache_dict.items():\n",
    "    if \".0.\" in activation_name or \"blocks\" not in activation_name:\n",
    "        print(activation_name, activation.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'NTK_by_parts_factor': 8.0,\n",
      " 'NTK_by_parts_high_freq_factor': 4.0,\n",
      " 'NTK_by_parts_low_freq_factor': 1.0,\n",
      " 'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_scale': 8.0,\n",
      " 'attn_scores_soft_cap': -1.0,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cuda'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'output_logits_soft_cap': -1.0,\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': False,\n",
      " 'trust_remote_code': False,\n",
      " 'ungroup_grouped_query_attention': False,\n",
      " 'use_NTK_by_parts_rope': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_normalization_before_and_after': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(reference_gpt2.cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config(d_model=768, debug=True, layer_norm_eps=1e-05, d_vocab=50257, init_range=0.02, n_ctx=1024, d_head=64, d_mlp=3072, n_heads=12, n_layers=12)\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class Config:\n",
    "    d_model: int = 768  # size of residual stream\n",
    "    debug: bool = True\n",
    "    layer_norm_eps: float = 1e-5\n",
    "    d_vocab: int = 50257\n",
    "    init_range: float = 0.02\n",
    "    n_ctx: int = 1024\n",
    "    d_head: int = 64\n",
    "    d_mlp: int = 3072\n",
    "    n_heads: int = 12\n",
    "    n_layers: int = 12\n",
    "    \n",
    "cfg = Config()\n",
    "print(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_float_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    \n",
    "    random_input = torch.randn(shape).cuda()\n",
    "    print(f\"input: {random_input.shape}\")\n",
    "    \n",
    "    output = layer(random_input)\n",
    "    print(f\"output: {output.shape}\")\n",
    "    \n",
    "    print(f\"output: {get_corner(output)}\") # prints first k(=3 default) elements from every axis\n",
    "    print(f\"-\"*10)\n",
    "    return output\n",
    "\n",
    "\n",
    "def rand_int_test(cls, shape):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    \n",
    "    random_input = torch.randint(100, 1000, shape).cuda()\n",
    "    print(f\"input: {random_input.shape}\")\n",
    "    \n",
    "    output = layer(random_input)\n",
    "    print(f\"output: {output.shape}\")\n",
    "    \n",
    "    print(f\"output: {get_corner(output)}\")\n",
    "    print(f\"-\"*10)\n",
    "    return output\n",
    "\n",
    "\n",
    "def load_gpt2_test(cls, gpt2_layer, input_name, cache_dict=cache.cache_dict):\n",
    "    cfg = Config(debug=True)\n",
    "    layer = cls(cfg).cuda()\n",
    "    layer.load_state_dict(\n",
    "        gpt2_layer.state_dict(), \n",
    "        strict=False\n",
    "    )\n",
    "    \n",
    "    if isinstance(input_name, str):\n",
    "        reference_input = cache_dict[input_name]\n",
    "    else:\n",
    "        reference_input = input_name\n",
    "    print(f\"input: {reference_input.shape}\")\n",
    "        \n",
    "    output = layer(reference_input)\n",
    "    print(f\"output: {output.shape}\")\n",
    "        \n",
    "    if str(gpt2_layer).startswith(\"Attention\"):\n",
    "        reference_output = gpt2_layer(reference_input, reference_input, reference_input)\n",
    "    else:\n",
    "        reference_output = gpt2_layer(reference_input)\n",
    "        \n",
    "    print(f\"reference_output: {reference_output.shape}\")\n",
    "        \n",
    "    comparison = torch.isclose(\n",
    "        output, reference_output, atol=1e-4, rtol=1e-3\n",
    "    )\n",
    "    print(f\"{comparison.sum()/comparison.numel():.2%} of the values are correct\")\n",
    "    \n",
    "    print(f\"output: {get_corner(output)}\")\n",
    "    print(f\"-\"*10)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### layernorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- make mean 0\n",
    "- normalize to have variance 1\n",
    "- scale with learned weights\n",
    "- translate with learned bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.w = nn.Parameter(torch.ones(self.cfg.d_model))\n",
    "        self.b = nn.Parameter(torch.zeros(self.cfg.d_model))\n",
    "    \n",
    "    def forward(self, residual):\n",
    "        # residual: [batch, position, d_model]\n",
    "        if self.cfg.debug: \n",
    "            print(f\"residual: {residual.shape}\")\n",
    "        residual = residual - einops.reduce(\n",
    "            residual,\n",
    "            \"batch position d_model -> batch position 1\", \n",
    "            \"mean\"\n",
    "        )\n",
    "        \n",
    "        # calculate the variance, square root it, add in an epsilon \n",
    "        scale = (\n",
    "        einops.reduce(\n",
    "            residual.pow(2),\n",
    "            \"batch position d_model -> batch position 1\", \n",
    "            \"mean\"\n",
    "        ) + cfg.layer_norm_eps).sqrt()\n",
    "        \n",
    "        # scale\n",
    "        normalized = residual / scale\n",
    "        \n",
    "        # normalized: [batch, position, d_model]\n",
    "        normalized = normalized * self.w + self.b\n",
    "        \n",
    "        if self.cfg.debug: \n",
    "            print(f\"normalized: {normalized.shape}\")\n",
    "        return normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[ 0.5619,  0.9242, -1.0676],\n",
      "         [ 0.7731, -0.1161, -1.2591],\n",
      "         [ 0.4798,  2.0295,  0.9904]],\n",
      "\n",
      "        [[ 0.6936,  0.3799, -0.3563],\n",
      "         [-0.3409, -1.8706,  1.9035],\n",
      "         [ 0.0347,  0.7611, -0.2391]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[-0.0667,  0.0881, -0.3085],\n",
      "         [ 0.0278, -0.2843,  0.2504],\n",
      "         [-0.5468, -0.5119, -0.6429]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(LayerNorm, [2, 4, 768])\n",
    "_ = load_gpt2_test(LayerNorm, reference_gpt2.ln_final, \"blocks.11.hook_resid_post\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- basically a lookup table from tokens to residual stream vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_E = nn.Parameter(torch.empty((self.cfg.d_vocab, self.cfg.d_model)))\n",
    "        nn.init.normal_(self.W_E, std=self.cfg.init_range)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        # tokens: [batch, position]\n",
    "        if self.cfg.debug:\n",
    "            print(f\"tokens: {tokens.shape}\")\n",
    "        \n",
    "        # embed: [batch, position, d_model]\n",
    "        embed = self.W_E[tokens, :]\n",
    "        \n",
    "        if self.cfg.debug:\n",
    "            print(f\"embed: {embed.shape}\")\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4])\n",
      "tokens: torch.Size([2, 4])\n",
      "embed: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[ 0.0012, -0.0255, -0.0062],\n",
      "         [-0.0118,  0.0017, -0.0253],\n",
      "         [ 0.0072,  0.0229, -0.0031]],\n",
      "\n",
      "        [[ 0.0195,  0.0319, -0.0237],\n",
      "         [-0.0006, -0.0248, -0.0095],\n",
      "         [ 0.0308, -0.0121, -0.0093]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35])\n",
      "tokens: torch.Size([1, 35])\n",
      "embed: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[ 0.0514, -0.0277,  0.0499],\n",
      "         [ 0.1474, -0.0959,  0.1430],\n",
      "         [ 0.1596, -0.1249,  0.1148]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_int_test(Embed, [2, 4])\n",
    "_ = load_gpt2_test(Embed, reference_gpt2.embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### positional embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PosEmbed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_pos = nn.Parameter(torch.empty((self.cfg.n_ctx, self.cfg.d_model)))\n",
    "        nn.init.normal(self.W_pos, std=self.cfg.init_range)\n",
    "    \n",
    "    def forward(self, tokens):\n",
    "        # tokens = [batch, position]\n",
    "        if self.cfg.debug:\n",
    "            print(f\"tokens: {tokens.shape}\")\n",
    "        \n",
    "        # pos_embed = [position, d_model]\n",
    "        pos_embed = self.W_pos[:tokens.size(1), :]\n",
    "        \n",
    "        # pos_embed = [batch, position, d_model]\n",
    "        pos_embed = einops.repeat(\n",
    "            pos_embed,\n",
    "            \"position d_model -> batch position d_model\",\n",
    "            batch=tokens.size(0)\n",
    "        )\n",
    "        \n",
    "        if self.cfg.debug:\n",
    "            print(f\"pos_embed: {pos_embed.shape}\")\n",
    "        return pos_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4])\n",
      "tokens: torch.Size([2, 4])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[ 0.0063,  0.0192,  0.0057],\n",
      "         [ 0.0129,  0.0290, -0.0277],\n",
      "         [-0.0143,  0.0313, -0.0219]],\n",
      "\n",
      "        [[ 0.0063,  0.0192,  0.0057],\n",
      "         [ 0.0129,  0.0290, -0.0277],\n",
      "         [-0.0143,  0.0313, -0.0219]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35])\n",
      "tokens: torch.Size([1, 35])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[-0.0188, -0.1974,  0.0040],\n",
      "         [ 0.0240, -0.0538, -0.0949],\n",
      "         [ 0.0042, -0.0848,  0.0545]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_int_test(PosEmbed, [2, 4])\n",
    "_ = load_gpt2_test(PosEmbed, reference_gpt2.pos_embed, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* step 1: produce an attention pattern - for each destination tokens, probability distribution over previous tokens (incl current token)\n",
    "    - linear map from input -> query, key. shape = [batch, position, n_heads, d_head]\n",
    "    - dot product every pair of queries and keys to get attn_scores. shape = [batch, n_heads, query_pos, key_pos] (query = dest, key = source)\n",
    "    - scale and mask attn_scores to make it lower triangular, ie causal\n",
    "    - softmax row-wise, to get a probability distribution along each of the key_pos dimension - this is our attention pattern!\n",
    "\n",
    "* step 2: move information from source tokens to destination token using attention pattern (move = apply linear map)\n",
    "    - linear map from input -> value. shape = [batch, key_pos, n_heads, d_head]\n",
    "    - mix along the key_pos with attn pattern to get z, a mixed value. shape of z = [batch, query_pos, n_heads, d_head]\n",
    "    - map to output. shape = [batch, position, d_model]. position = query_pos, we've summed over all heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pysvelte\n",
    "# pysvelte.AttentionMulti(\n",
    "#     tokens=reference_gpt2.to_str_tokens(reference_text), \n",
    "#     attention=cache['blocks.0.attn.hook_attn'][0].permute(1, 2, 0)\n",
    "# ).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        \n",
    "        self.W_Q = nn.Parameter(torch.empty((self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)))\n",
    "        nn.init.normal(self.W_Q, std=self.cfg.init_range)\n",
    "        self.b_Q = nn.Parameter(torch.zeros((self.cfg.n_heads, self.cfg.d_head)))\n",
    "        \n",
    "        self.W_K = nn.Parameter(torch.empty((self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)))\n",
    "        nn.init.normal(self.W_K, std=self.cfg.init_range)\n",
    "        self.b_K = nn.Parameter(torch.zeros((self.cfg.n_heads, self.cfg.d_head)))\n",
    "        \n",
    "        self.W_V = nn.Parameter(torch.empty((self.cfg.n_heads, self.cfg.d_model, self.cfg.d_head)))\n",
    "        nn.init.normal(self.W_V, std=self.cfg.init_range)\n",
    "        self.b_V = nn.Parameter(torch.zeros((self.cfg.n_heads, self.cfg.d_head)))\n",
    "        \n",
    "        self.W_O = nn.Parameter(torch.empty((self.cfg.n_heads, self.cfg.d_head, self.cfg.d_model)))\n",
    "        nn.init.normal(self.W_O, std=self.cfg.init_range)\n",
    "        self.b_O = nn.Parameter(torch.zeros((self.cfg.d_model)))\n",
    "        \n",
    "        self.register_buffer(\n",
    "            \"IGNORE\",\n",
    "            torch.tensor(-1e5, dtype=torch.float32, device=\"cuda\")\n",
    "        )\n",
    "        \n",
    "    def forward(self, normalized_resid_pre):\n",
    "        # normalized_resid_pre: [batch, position, d_model]\n",
    "        if self.cfg.debug:\n",
    "            print(f\"normalized_resid_pre: {normalized_resid_pre.shape}\")\n",
    "            \n",
    "        q = einsum(\n",
    "            \"batch query_pos d_model, n_heads d_model d_head -> batch query_pos n_heads d_head\",\n",
    "            normalized_resid_pre,\n",
    "            self.W_Q\n",
    "        ) + self.b_Q\n",
    "        k = einsum(\n",
    "            \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "            normalized_resid_pre,\n",
    "            self.W_K\n",
    "        ) + self.b_K\n",
    "        \n",
    "        attn_scores = einsum(\n",
    "            \"batch query_pos n_heads d_model, batch key_pos n_heads d_model -> batch n_heads query_pos key_pos\",\n",
    "            q,\n",
    "            k\n",
    "        )\n",
    "        attn_scores = attn_scores / math.sqrt(self.cfg.d_head)\n",
    "        attn_scores = self.apply_causal_nask(attn_scores)\n",
    "        \n",
    "        pattern = attn_scores.softmax(dim=-1)\n",
    "        \n",
    "        v = einsum(\n",
    "            \"batch key_pos d_model, n_heads d_model d_head -> batch key_pos n_heads d_head\",\n",
    "            normalized_resid_pre,\n",
    "            self.W_V\n",
    "        ) + self.b_V\n",
    "        \n",
    "        z = einsum(\n",
    "            \"batch n_heads query_pos key_pos, batch key_pos n_heads d_head -> batch query_pos n_heads d_head\", \n",
    "            pattern, \n",
    "            v\n",
    "        )\n",
    "\n",
    "        attn_out = einsum(\n",
    "            \"batch query_pos n_heads d_head, n_heads d_head d_model -> batch query_pos d_model\", \n",
    "            z, \n",
    "            self.W_O\n",
    "        ) + self.b_O\n",
    "        return attn_out\n",
    "                \n",
    "      \n",
    "    def apply_causal_nask(self, attn_scores):\n",
    "        # attn_scores: [batch, n_heads, query_pos, key_pos]  \n",
    "        mask = torch.triu(torch.ones(attn_scores.size(-2), attn_scores.size(-1), device=attn_scores.device), diagonal=1).bool()\n",
    "        attn_scores.masked_fill_(mask, self.IGNORE)\n",
    "        return attn_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[-0.2244,  0.4576, -0.2527],\n",
      "         [-0.3080,  0.2571,  0.0214],\n",
      "         [-0.0862,  0.0386,  0.0249]],\n",
      "\n",
      "        [[-0.0063, -0.6488, -0.0961],\n",
      "         [ 0.0497, -0.3078,  0.0510],\n",
      "         [ 0.0682, -0.0271, -0.0599]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[ 0.7966,  0.0170,  0.0348],\n",
      "         [ 0.0013,  0.1575, -0.1406],\n",
      "         [ 0.0897, -0.7241, -0.6987]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(Attention, [2, 4, 768])\n",
    "_ = load_gpt2_test(cls=Attention, gpt2_layer=reference_gpt2.blocks[0].attn, input_name=cache[\"blocks.0.ln1.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_in = nn.Parameter(torch.empty((self.cfg.d_model, self.cfg.d_mlp)))\n",
    "        nn.init.normal_(self.W_in, std=self.cfg.init_range)\n",
    "        self.b_in = nn.Parameter(torch.zeros((self.cfg.d_mlp)))\n",
    "        \n",
    "        self.W_out = nn.Parameter(torch.empty((self.cfg.d_mlp, self.cfg.d_model)))\n",
    "        nn.init.normal_(self.W_out, std=self.cfg.init_range)\n",
    "        self.b_out = nn.Parameter(torch.zeros((self.cfg.d_model)))\n",
    "        \n",
    "    def forward(self, normalized_resid_mid):\n",
    "        # normalized_resid_mid: [batch, position, d_model]\n",
    "        if self.cfg.debug:\n",
    "            print(f\"normalized_resid_mid: {normalized_resid_mid.shape}\")\n",
    "        \n",
    "        pre = einsum(\n",
    "            \"batch position d_model, d_model d_mlp -> batch position d_mlp\",\n",
    "            normalized_resid_mid,\n",
    "            self.W_in\n",
    "        ) + self.b_in\n",
    "        \n",
    "        post = gelu_new(pre)\n",
    "\n",
    "        mlp_out = einsum(\n",
    "            \"batch position d_mlp, d_mlp d_model -> batch position d_model\",\n",
    "            post,\n",
    "            self.W_out \n",
    "        ) + self.b_out\n",
    "        \n",
    "        if self.cfg.debug:\n",
    "            print(f\"mlp_out: {mlp_out.shape}\")\n",
    "        return mlp_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[ 0.1754,  0.1624, -0.2302],\n",
      "         [-0.4422, -0.4709,  0.0976],\n",
      "         [ 0.2639, -0.2979,  0.2109]],\n",
      "\n",
      "        [[ 0.0609,  0.2412,  0.2392],\n",
      "         [ 0.1290, -0.1086,  0.1249],\n",
      "         [ 0.1668, -0.4153,  0.3428]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[-0.4380,  0.3624,  0.5117],\n",
      "         [-1.0766, -0.0438,  0.3276],\n",
      "         [-1.2182, -1.5481, -0.9702]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(MLP, [2, 4, 768])\n",
    "_ = load_gpt2_test(MLP, reference_gpt2.blocks[0].mlp, cache[\"blocks.0.ln2.hook_normalized\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### transformer block\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.ln1 = LayerNorm(cfg)\n",
    "        self.attn = Attention(cfg)\n",
    "        self.ln2 = LayerNorm(cfg)\n",
    "        self.mlp = MLP(cfg)\n",
    "    \n",
    "    def forward(self, resid_pre):\n",
    "        # resid_pre = [batch, position, d_model]\n",
    "        normalized_resid_pre = self.ln1(resid_pre)\n",
    "        attn_out = self.attn(normalized_resid_pre)\n",
    "        resid_mid = resid_pre + attn_out\n",
    "        \n",
    "        normalized_resid_mid = self.ln2(resid_mid)\n",
    "        mlp_out = self.mlp(normalized_resid_mid)\n",
    "        resid_post = resid_mid + mlp_out\n",
    "        \n",
    "        return resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "output: torch.Size([2, 4, 768])\n",
      "output: tensor([[[ 0.3182, -1.0828, -0.7474],\n",
      "         [-0.5555, -0.1945,  0.5684],\n",
      "         [ 1.9297,  0.7157, -0.6533]],\n",
      "\n",
      "        [[-0.8014,  0.2286,  1.2266],\n",
      "         [-0.1060,  0.3313, -0.3862],\n",
      "         [-0.5618, -1.1045,  0.7777]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "output: torch.Size([1, 35, 768])\n",
      "reference_output: torch.Size([1, 35, 768])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[ 0.3911,  0.1543,  0.6005],\n",
      "         [-0.9039, -0.0360,  0.2351],\n",
      "         [-0.9647, -2.4819, -1.4995]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "_ = rand_float_test(TransformerBlock, [2, 4, 768])\n",
    "_ = load_gpt2_test(TransformerBlock, reference_gpt2.blocks[0], cache[\"resid_pre\", 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unembedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4, 768])\n",
      "normalized_resid_final: torch.Size([2, 4, 768])\n",
      "logits: torch.Size([2, 4, 50257])\n",
      "output: torch.Size([2, 4, 50257])\n",
      "output: tensor([[[-0.5075, -0.8049, -1.0693],\n",
      "         [ 0.0154,  0.1165, -0.6128],\n",
      "         [ 0.2335,  0.4006, -0.2663]],\n",
      "\n",
      "        [[-1.2108, -0.1579,  0.1332],\n",
      "         [-0.4549, -0.6426,  0.3444],\n",
      "         [-0.0937,  0.2746,  0.3755]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35, 768])\n",
      "normalized_resid_final: torch.Size([1, 35, 768])\n",
      "logits: torch.Size([1, 35, 50257])\n",
      "output: torch.Size([1, 35, 50257])\n",
      "reference_output: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[ -43.4317,  -39.8364,  -43.0659],\n",
      "         [-128.0392, -127.9935, -130.7010],\n",
      "         [-119.8521, -121.0064, -123.8819]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "class Unembed(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.W_U = nn.Parameter(torch.empty((self.cfg.d_model, self.cfg.d_vocab)))\n",
    "        nn.init.normal_(self.W_U, std=self.cfg.init_range)\n",
    "        self.b_U = nn.Parameter(torch.zeros((self.cfg.d_vocab), requires_grad=False))\n",
    "        \n",
    "    def forward(self, normalized_resid_final):\n",
    "        # normalized_resid_final = [batch position, d_model]\n",
    "        if self.cfg.debug:\n",
    "            print(f'normalized_resid_final: {normalized_resid_final.shape}')\n",
    "        \n",
    "        # logits = [batch, position, d_vocab]\n",
    "        logits = einsum(\n",
    "            \"batch position d_model, d_model d_vocab -> batch position d_vocab\",\n",
    "            normalized_resid_final,\n",
    "            self.W_U\n",
    "        ) + self.b_U\n",
    "        \n",
    "        if self.cfg.debug:\n",
    "            print(f'logits: {logits.shape}')\n",
    "        return logits\n",
    "\n",
    "_ = rand_float_test(Unembed, [2, 4, 768])\n",
    "_ = load_gpt2_test(Unembed, reference_gpt2.unembed, cache[\"ln_final.hook_normalized\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### full transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: torch.Size([2, 4])\n",
      "tokens: torch.Size([2, 4])\n",
      "embed: torch.Size([2, 4, 768])\n",
      "tokens: torch.Size([2, 4, 768])\n",
      "pos_embed: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_pre: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_mid: torch.Size([2, 4, 768])\n",
      "mlp_out: torch.Size([2, 4, 768])\n",
      "residual: torch.Size([2, 4, 768])\n",
      "normalized: torch.Size([2, 4, 768])\n",
      "normalized_resid_final: torch.Size([2, 4, 768])\n",
      "logits: torch.Size([2, 4, 50257])\n",
      "output: torch.Size([2, 4, 50257])\n",
      "output: tensor([[[-0.1812,  0.0404,  0.2850],\n",
      "         [-0.3998,  0.5342,  0.7475],\n",
      "         [-0.2371,  0.3719,  0.6565]],\n",
      "\n",
      "        [[-0.5418,  0.0402,  0.1755],\n",
      "         [-0.4131, -0.0460,  0.4652],\n",
      "         [-0.1784, -0.3084,  0.1760]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n",
      "input: torch.Size([1, 35])\n",
      "tokens: torch.Size([1, 35])\n",
      "embed: torch.Size([1, 35, 768])\n",
      "tokens: torch.Size([1, 35, 768])\n",
      "pos_embed: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_pre: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_mid: torch.Size([1, 35, 768])\n",
      "mlp_out: torch.Size([1, 35, 768])\n",
      "residual: torch.Size([1, 35, 768])\n",
      "normalized: torch.Size([1, 35, 768])\n",
      "normalized_resid_final: torch.Size([1, 35, 768])\n",
      "logits: torch.Size([1, 35, 50257])\n",
      "output: torch.Size([1, 35, 50257])\n",
      "reference_output: torch.Size([1, 35, 50257])\n",
      "100.00% of the values are correct\n",
      "output: tensor([[[ -43.4317,  -39.8364,  -43.0659],\n",
      "         [-128.0392, -127.9935, -130.7010],\n",
      "         [-119.8521, -121.0064, -123.8819]]], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "class DemoTransformer(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.cfg = cfg\n",
    "        self.embed = Embed(cfg)\n",
    "        self.pos_embed = PosEmbed(cfg)\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [TransformerBlock(cfg) for _ in range(self.cfg.n_layers)]\n",
    "        )\n",
    "        self.ln_final = LayerNorm(cfg)\n",
    "        self.unembed = Unembed(cfg)\n",
    "        \n",
    "    def forward(self, tokens):\n",
    "        # tokens = [batch, position]\n",
    "        embed = self.embed(tokens)\n",
    "        pos_embed = self.pos_embed(embed)\n",
    "        residual = embed + pos_embed # shape = [batch, position, d_model]\n",
    "        for block in self.blocks:\n",
    "            residual = block(residual)\n",
    "        normalized_resid_final = self.ln_final(residual) # shape = [batch, position, d_model]\n",
    "        logits = self.unembed(normalized_resid_final) # shape = [batch, position, d_vocab]\n",
    "        return logits\n",
    "\n",
    "_ = rand_int_test(DemoTransformer, [2, 4])\n",
    "_ = load_gpt2_test(DemoTransformer, reference_gpt2, tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DemoTransformer(\n",
       "  (embed): Embed()\n",
       "  (pos_embed): PosEmbed()\n",
       "  (blocks): ModuleList(\n",
       "    (0-11): 12 x TransformerBlock(\n",
       "      (ln1): LayerNorm()\n",
       "      (attn): Attention()\n",
       "      (ln2): LayerNorm()\n",
       "      (mlp): MLP()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): LayerNorm()\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo_gpt2 = DemoTransformer(Config(debug=True))\n",
    "demo_gpt2.load_state_dict(reference_gpt2.state_dict(), strict=False)\n",
    "demo_gpt2.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"\"\"Mini scule is a species of microhylid frog endemic to Madagascar that was described in 2019. The scientific name of the species refers to its size, being a pun on the word minuscule. It is very small, measuring only 8.4 to 10.8 mm (0.33 to 0.43 in) in snout–vent length. It has bronze underparts with a brown groin and back of the thigh, cream upperparts with brown flecking, a dark brown side of the head, and a red iris. On the hind feet, the first toe is absent and the second and fifth toes are strongly reduced. The frog is known only from the Sainte Luce Reserve, where it inhabits areas with deep leaf litter near semi-permanent water bodies. Specimens of frogs from Mandena, the Vohimena mountains, the southern Anosy Mountains, and Tsitongambarika may also be of this species. Along with Mini mum and Mini ature, the other two species in its genus, it received media attention when first described due to the wordplay in its scientific name. (Full article...)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokens: torch.Size([1, 237])\n",
      "embed: torch.Size([1, 237, 768])\n",
      "tokens: torch.Size([1, 237, 768])\n",
      "pos_embed: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_pre: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_mid: torch.Size([1, 237, 768])\n",
      "mlp_out: torch.Size([1, 237, 768])\n",
      "residual: torch.Size([1, 237, 768])\n",
      "normalized: torch.Size([1, 237, 768])\n",
      "normalized_resid_final: torch.Size([1, 237, 768])\n",
      "logits: torch.Size([1, 237, 50257])\n"
     ]
    }
   ],
   "source": [
    "test_tokens = reference_gpt2.to_tokens(test_string).cuda()\n",
    "demo_logits = demo_gpt2(test_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.7186, device='cuda:0', grad_fn=<NegBackward0>)\n",
      "Loss as average prob tensor(0.0243, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Loss as 'uniform over this many variables' tensor(41.2079, device='cuda:0', grad_fn=<ExpBackward0>)\n",
      "Uniform loss over the vocab 10.82490511970208\n"
     ]
    }
   ],
   "source": [
    "def lm_cross_entropy_loss(logits, tokens):\n",
    "    # Measure next token loss\n",
    "    # Logits have shape [batch, position, d_vocab]\n",
    "    # Tokens have shape [batch, position]\n",
    "    log_probs = logits.log_softmax(dim=-1)\n",
    "    pred_log_probs = log_probs[:, :-1].gather(dim=-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    return -pred_log_probs.mean()\n",
    "loss = lm_cross_entropy_loss(demo_logits, test_tokens)\n",
    "print(loss)\n",
    "print(\"Loss as average prob\", (-loss).exp())\n",
    "print(\"Loss as 'uniform over this many variables'\", (loss).exp())\n",
    "print(\"Uniform loss over the vocab\", math.log(demo_gpt2.cfg.d_vocab))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
