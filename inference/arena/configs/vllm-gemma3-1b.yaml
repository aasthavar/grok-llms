framework: vllm
gpu: g6e

model:
  model_id: google/gemma-3-1b-it
  tokenizer: auto
  dtype: bfloat16
  quantization: awq

dataset:
  file: data/sharegpt.json
  template: chatml

experiments:
  - name: c1_r30_latency
    benchmark:
      concurrency: 1
      request_rate: 20
    serve:
      enforce_eager: true
      max_model_len: 4096
      trust_remote_code: true
      generation_config:
        temperature: 0.7
        top_p: 0.9
        max_tokens: 128

  - name: c5_r30_latency
    benchmark:
      concurrency: 1
      request_rate: 20
    serve:
      enforce_eager: true
      max_model_len: 4096
      trust_remote_code: true
      generation_config:
        temperature: 0.7
        top_p: 0.9
        max_tokens: 128

  - name: c1_r75_latency
    benchmark:
      concurrency: 1
      request_rate: 20
    serve:
      enforce_eager: true
      max_model_len: 4096
      trust_remote_code: true
      generation_config:
        temperature: 0.7
        top_p: 0.9
        max_tokens: 128