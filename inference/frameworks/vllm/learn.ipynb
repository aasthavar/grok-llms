{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e63561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda = grok"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c2eb52",
   "metadata": {},
   "source": [
    "# benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9def895",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "    {\n",
    "        \"model_id\": \"google/gemma-3-1b-it\",\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"deepseek-ai/DeepSeek-R1-0528-Qwen3-8B\",\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"Qwen/QwQ-32B\",\n",
    "    },\n",
    "    {\n",
    "        \"model_id\": \"mistralai/Devstral-Small-2505\",\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09a7f11b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "693612f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(\"/home/ubuntu/grok-llms/inference/arena/data/ShareGPT_V3_unfiltered_cleaned_split.json\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a9697533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 'QWJhYvA_0',\n",
       " 'conversations': [{'from': 'human',\n",
       "   'value': \"Summarize the main ideas of Jeff Walker's Product Launch Formula into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Here are the main ideas of Jeff Walker's Product Launch Formula that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify the target audience and their needs: Understand the ideal customer for the product or service, and create a messaging that resonates with them.\\n2. Pre-launch: Build anticipation and excitement for the launch by creating buzz, gathering testimonials and case studies, and using social media to create awareness.\\n3. Launch: Use a well-crafted launch sequence to maximize sales and conversions. This can include offering bonuses, creating scarcity, and using a deadline to create urgency.\\n4. Post-launch: Follow up with customers, gather feedback, and continue to provide value to keep them engaged and loyal.\\n5. Create a product suite: Once the initial product is successful, expand the offering by creating additional products or services that address other needs of the same audience.\\n6. Continual optimization: Continually monitor and optimize the launch process and product suite to improve results.\\n7. Build a community: Use the launch process to build a community of customers who are passionate about the product and can help spread the word.\\n8. Use automation: Use technology and automation to streamline the launch process and improve efficiency.\"},\n",
       "  {'from': 'human',\n",
       "   'value': \"Summarize the main ideas of Brendon Burchard's Experts Academy into bullet points as it pertains to a growth marketing agency implementing these strategies and tactics for their clients...\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Here are the main ideas of Brendon Burchard's Experts Academy that can be applied by a growth marketing agency for their clients:\\n\\n1. Identify your expertise: Define your area of expertise and create a unique value proposition that differentiates you from others.\\n2. Create a content strategy: Develop a content strategy that showcases your expertise and provides value to your audience. This can include blog posts, videos, podcasts, and social media content.\\n3. Build your audience: Use social media, advertising, and other channels to build your audience and increase your visibility.\\n4. Develop your products: Create products and services that align with your expertise and meet the needs of your audience. These can include digital products, courses, coaching, and consulting services.\\n5. Create a brand: Develop a strong brand that reflects your expertise and resonates with your audience.\\n6. Build authority: Establish yourself as an authority in your industry by speaking at events, publishing a book, or appearing on podcasts or TV shows.\\n7. Monetize your expertise: Develop a monetization strategy that leverages your expertise and products to generate revenue.\\n8. Build a team: As your business grows, build a team of experts to help you scale your business and provide excellent service to your clients.\\n9. Continual improvement: Continually improve your products, services, and marketing strategies to stay ahead of the competition and provide the best possible experience for your clients.\\n10. Focus on impact: Ultimately, focus on making a positive impact in the lives of your clients and using your expertise to make a difference in the world.\"},\n",
       "  {'from': 'human',\n",
       "   'value': 'What are the mental triggers in Jeff Walker\\'s Product Launch Formula and \"Launch\" book?'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Here are the mental triggers described in Jeff Walker\\'s Product Launch Formula and \"Launch\" book:\\n\\n1. Scarcity: Creating a sense of urgency by limiting the availability of the product or service being launched.\\n2. Authority: Establishing oneself as an expert or authority in the industry to build credibility and trust with the audience.\\n3. Social proof: Using testimonials, case studies, and other forms of social proof to demonstrate the effectiveness and value of the product or service.\\n4. Reciprocity: Offering something of value to the audience before asking for a sale or commitment.\\n5. Likeability: Building a personal connection with the audience and creating a likable persona to increase the chances of a sale.\\n6. Anticipation: Building anticipation and excitement leading up to the launch to generate buzz and increase interest in the product or service.\\n7. Exclusivity: Creating a feeling of exclusivity by offering early access or special deals to a select group of people.\\n8. Belonging: Creating a sense of community or belonging among the audience to increase engagement and loyalty to the brand.\\n9. Storytelling: Using storytelling to connect with the audience on an emotional level and make the product or service more relatable and memorable.\\n\\nBy leveraging these mental triggers, businesses can create an effective product launch strategy that resonates with their target audience and drives sales and growth.'},\n",
       "  {'from': 'human',\n",
       "   'value': 'Write a summary of why scarcity and urgency are the strongest mental triggers and have been the driving force behind many of our best performing campaigns over the last 8 years.'},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Scarcity and urgency are two of the strongest mental triggers in marketing because they create a sense of exclusivity and importance around a product or service. By creating a sense of limited availability or a deadline for a promotion, businesses can motivate customers to take action and make a purchase before it's too late.\\n\\nOver the last eight years, many businesses have leveraged these triggers in their marketing campaigns to great success. Scarcity and urgency have been used to sell everything from digital products to physical goods and services, and have proven to be highly effective in driving conversions and increasing revenue.\\n\\nThis is because scarcity and urgency tap into the natural human desire for instant gratification and fear of missing out (FOMO). When customers believe that they may miss out on a valuable opportunity if they don't act quickly, they are more likely to take action and make a purchase.\\n\\nFurthermore, scarcity and urgency can be combined with other mental triggers, such as social proof and authority, to create even more powerful marketing campaigns. By demonstrating the popularity and effectiveness of a product or service, businesses can increase the perceived value and urgency of a promotion, leading to even more sales and revenue.\\n\\nOverall, scarcity and urgency are powerful mental triggers that have been proven to drive results in marketing campaigns. By leveraging these triggers, businesses can create a sense of urgency and exclusivity around their products or services, motivating customers to take action and make a purchase before it's too late.\"},\n",
       "  {'from': 'human',\n",
       "   'value': \"Summarize Russell Brunson's Perfect Webinar Script...\"},\n",
       "  {'from': 'gpt',\n",
       "   'value': \"Russell Brunson's Perfect Webinar Script is a framework for delivering webinars that are designed to sell a product or service. Here are the key components of the Perfect Webinar Script:\\n\\n1. Introduction: The webinar starts with an introduction that establishes the presenter's credibility and establishes the topic.\\n2. Story: The presenter then tells a personal story that is related to the topic and establishes a connection with the audience.\\n3. Content: The presenter provides valuable content related to the topic, demonstrating their expertise and building trust with the audience.\\n4. Offer: The presenter then introduces the product or service they are selling, explaining how it solves the audience's problem and providing social proof to support their claims.\\n5. Bonuses: The presenter offers bonuses to sweeten the deal and make the offer more enticing.\\n6. Scarcity: The presenter creates a sense of urgency by limiting the availability of the offer or including a deadline for purchase.\\n7. FAQs: The presenter answers common questions about the offer, addressing objections and building trust with the audience.\\n8. Closing: The presenter then delivers a clear call to action, directing the audience to take a specific action to purchase the product or service.\\n\\nOverall, the Perfect Webinar Script is designed to build trust with the audience, establish the presenter's credibility, and provide valuable content, while also creating a sense of urgency and scarcity around the offer being presented. By following this framework, businesses can use webinars as a powerful tool to sell their products and services to a targeted audience.\"},\n",
       "  {'from': 'human',\n",
       "   'value': 'Summarize the 6 human needs as Tony Robbins explains...'},\n",
       "  {'from': 'gpt',\n",
       "   'value': 'Tony Robbins describes six core human needs that drive our behaviors and motivations. These six needs are:\\n\\n1. Certainty: The need for safety, stability, and predictability. This includes the need for comfort, security, and control over our environment.\\n2. Variety: The need for novelty, excitement, and change. This includes the need for adventure, stimulation, and new experiences.\\n3. Significance: The need to feel important, special, and unique. This includes the need for recognition, achievement, and respect.\\n4. Connection: The need for social connection and love. This includes the need for intimacy, belonging, and friendship.\\n5. Growth: The need for personal growth and development. This includes the need for learning, self-improvement, and progress.\\n6. Contribution: The need to make a positive impact in the world. This includes the need to give back, help others, and make a difference.\\n\\nAccording to Tony Robbins, these six needs are universal and apply to all individuals. Each person may prioritize these needs differently, and may fulfill them in different ways, but they are fundamental drivers of human behavior. By understanding these needs, individuals can gain insight into their own motivations and behaviors, and can use this knowledge to create a more fulfilling and meaningful life.'}]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_dataset_path = \"/home/ubuntu/grok-llms/inference/arena/data/ShareGPT_V3_unfiltered_cleaned_split.json\"\n",
    "with open(json_dataset_path, \"r\") as f:\n",
    "    data = json.load(f)\n",
    "    \n",
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beefeacf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "14397881",
   "metadata": {},
   "source": [
    "# multi-lora inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d0932fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login, snapshot_download\n",
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "\n",
    "HUGGINGFACE_TOKEN = \"\"\n",
    "login(token=HUGGINGFACE_TOKEN)\n",
    "\n",
    "# sudo chown -R $(whoami) ~/.cache/huggingface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d85184",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = LLM(\n",
    "    model=\"meta-llama/Llama-2-7b-hf\",\n",
    "    enable_lora=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379ee38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_lora_path = snapshot_download(\n",
    "    repo_id=\"yard1/llama-2-7b-sql-lora-test\"\n",
    ")\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "    temperature=0.0,\n",
    "    max_tokens=64,\n",
    "    stop=[\"[/assistant]\"]\n",
    ")\n",
    "\n",
    "prompts = [\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_74 (icao VARCHAR, airport VARCHAR)\\n\\n question: Name the ICAO for lilongwe international airport [/user] [assistant]\",\n",
    "     \"[user] Write a SQL query to answer the question based on the table schema.\\n\\n context: CREATE TABLE table_name_11 (nationality VARCHAR, elector VARCHAR)\\n\\n question: When Anchero Pantaleone was the elector what is under nationality? [/user] [assistant]\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfcc8c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(os.listdir(sql_lora_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b872a235",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = llm.generate(\n",
    "    prompts=prompts,\n",
    "    sampling_params=sampling_params,\n",
    "    lora_request=LoRARequest(\"sql_adapter\", 1, sql_lora_path)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce24746d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[0].outputs[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa39500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs[1].outputs[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d4e10a",
   "metadata": {},
   "source": [
    "### serve lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ea314f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: do an ls for $HOME/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots and verify the snapshot id\n",
    "\n",
    "# terminal 1\n",
    "vllm serve meta-llama/Llama-2-7b-hf \\\n",
    "    --enable-lora \\\n",
    "    --lora-modules '{\"name\": \"sql-lora\", \"path\": \"/home/ubuntu/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c\", \"base_model_name\": \"meta-llama/Llama-2-7b\"}'\n",
    "    \n",
    "\n",
    "# terminal 2\n",
    "curl http://localhost:8000/v1/models\n",
    "\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"sql-lora\",\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"max_tokens\": 7,\n",
    "        \"temperature\": 0\n",
    "    }' | jq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d446796",
   "metadata": {},
   "source": [
    "### dynamically serve lora adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affa7454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# terminal 1\n",
    "export VLLM_ALLOW_RUNTIME_LORA_UPDATING=True\n",
    "vllm serve meta-llama/Llama-2-7b-hf --enable-lora\n",
    "\n",
    "# terminal 2\n",
    "curl -X POST http://localhost:8000/v1/load_lora_adapter \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"lora_name\": \"sql_adapter\",\n",
    "    \"lora_path\": \"/home/ubuntu/.cache/huggingface/hub/models--yard1--llama-2-7b-sql-lora-test/snapshots/0dfa347e8877a4d4ed19ee56c140fa518470028c\"\n",
    "}'\n",
    "\n",
    "curl http://localhost:8000/v1/completions \\\n",
    "    -H \"Content-Type: application/json\" \\\n",
    "    -d '{\n",
    "        \"model\": \"sql_adapter\",\n",
    "        \"prompt\": \"San Francisco is a\",\n",
    "        \"max_tokens\": 7,\n",
    "        \"temperature\": 0\n",
    "    }' | jq\n",
    "\n",
    "curl -X POST http://localhost:8000/v1/unload_lora_adapter \\\n",
    "-H \"Content-Type: application/json\" \\\n",
    "-d '{\n",
    "    \"lora_name\": \"sql_adapter\"\n",
    "}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36271225",
   "metadata": {},
   "source": [
    "### lora with quantization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d065e622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install bitsandbytes --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af5da315",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/grok/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 16:58:43 [__init__.py:239] Automatically detected platform cuda.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 16:58:45,294\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "from typing import Optional\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "from vllm import EngineArgs, LLMEngine, RequestOutput, SamplingParams\n",
    "from vllm.lora.request import LoRARequest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b242cd54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_prompts(\n",
    "        lora_path: str\n",
    ") -> list[tuple[str, SamplingParams, Optional[LoRARequest]]]:\n",
    "    return [\n",
    "        # this is an example of using quantization without LoRA\n",
    "        (\"My name is\",\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        logprobs=1,\n",
    "                        prompt_logprobs=1,\n",
    "                        max_tokens=128), None),\n",
    "        # the next three examples use quantization with LoRA\n",
    "        (\"my name is\",\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        logprobs=1,\n",
    "                        prompt_logprobs=1,\n",
    "                        max_tokens=128),\n",
    "         LoRARequest(\"lora-test-1\", 1, lora_path)),\n",
    "        (\"The capital of USA is\",\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        logprobs=1,\n",
    "                        prompt_logprobs=1,\n",
    "                        max_tokens=128),\n",
    "         LoRARequest(\"lora-test-2\", 1, lora_path)),\n",
    "        (\"The capital of France is\",\n",
    "         SamplingParams(temperature=0.0,\n",
    "                        logprobs=1,\n",
    "                        prompt_logprobs=1,\n",
    "                        max_tokens=128),\n",
    "         LoRARequest(\"lora-test-3\", 1, lora_path)),\n",
    "    ]\n",
    "\n",
    "\n",
    "def process_requests(engine: LLMEngine,\n",
    "                     test_prompts: list[tuple[str, SamplingParams,\n",
    "                                              Optional[LoRARequest]]]):\n",
    "    \"\"\"Continuously process a list of prompts and handle the outputs.\"\"\"\n",
    "    request_id = 0\n",
    "\n",
    "    while test_prompts or engine.has_unfinished_requests():\n",
    "        if test_prompts:\n",
    "            prompt, sampling_params, lora_request = test_prompts.pop(0)\n",
    "            engine.add_request(str(request_id),\n",
    "                               prompt,\n",
    "                               sampling_params,\n",
    "                               lora_request=lora_request)\n",
    "            request_id += 1\n",
    "\n",
    "        request_outputs: list[RequestOutput] = engine.step()\n",
    "        for request_output in request_outputs:\n",
    "            if request_output.finished:\n",
    "                print(\"----------------------------------------------------\")\n",
    "                print(f\"Prompt: {request_output.prompt}\")\n",
    "                print(f\"Output: {request_output.outputs[0].text}\")\n",
    "\n",
    "\n",
    "def initialize_engine(model: str, quantization: str,\n",
    "                      lora_repo: Optional[str]) -> LLMEngine:\n",
    "    \"\"\"Initialize the LLMEngine.\"\"\"\n",
    "\n",
    "    engine_args = EngineArgs(model=model,\n",
    "                             quantization=quantization,\n",
    "                             enable_lora=True,\n",
    "                             max_lora_rank=64,\n",
    "                             max_loras=4)\n",
    "    return LLMEngine.from_engine_args(engine_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f44b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_configs = [\n",
    "    # QLoRA (https://arxiv.org/abs/2305.14314)\n",
    "    # {\n",
    "    #     \"name\": \"qlora_inference_example\",\n",
    "    #     'model': \"huggyllama/llama-7b\",\n",
    "    #     'quantization': \"bitsandbytes\",\n",
    "    #     'lora_repo': 'timdettmers/qlora-flan-7b'\n",
    "    # },\n",
    "    # {\n",
    "    #     \"name\": \"AWQ_inference_with_lora_example\",\n",
    "    #     'model': 'TheBloke/TinyLlama-1.1B-Chat-v0.3-AWQ',\n",
    "    #     'quantization': \"awq\",\n",
    "    #     'lora_repo': 'jashing/tinyllama-colorist-lora'\n",
    "    # },\n",
    "    {\n",
    "        \"name\": \"GPTQ_inference_with_lora_example\",\n",
    "        'model': 'TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ',\n",
    "        'quantization': \"gptq\",\n",
    "        'lora_repo': 'jashing/tinyllama-colorist-lora'\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3216fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "~~~~~~~~~~~~~~~~ Running: GPTQ_inference_with_lora_example ~~~~~~~~~~~~~~~~\n",
      "INFO 05-23 16:58:51 [config.py:717] This model supports multiple tasks: {'generate', 'embed', 'score', 'reward', 'classify'}. Defaulting to 'generate'.\n",
      "INFO 05-23 16:58:52 [gptq_marlin.py:147] Detected that the model can run with gptq_marlin, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_marlin for faster inference\n",
      "INFO 05-23 16:58:52 [gptq_bitblas.py:168] Detected that the model can run with gptq_bitblas, however you specified quantization=gptq explicitly, so forcing gptq. Use quantization=gptq_bitblas for faster inference\n",
      "WARNING 05-23 16:58:52 [config.py:830] gptq quantization is not fully optimized yet. The speed can be slower than non-quantized models.\n",
      "INFO 05-23 16:58:52 [config.py:2003] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "WARNING 05-23 16:58:54 [utils.py:2382] We must use the `spawn` multiprocessing start method. Overriding VLLM_WORKER_MULTIPROC_METHOD to 'spawn'. See https://docs.vllm.ai/en/latest/getting_started/troubleshooting.html#python-multiprocessing for more information. Reason: CUDA is initialized\n",
      "INFO 05-23 16:58:57 [__init__.py:239] Automatically detected platform cuda.\n",
      "INFO 05-23 16:58:59 [core.py:58] Initializing a V1 LLM engine (v0.8.5.post1) with config: model='TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ', speculative_config=None, tokenizer='TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=gptq, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='auto', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"level\":3,\"custom_ops\":[\"none\"],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\"],\"use_inductor\":true,\"compile_sizes\":[],\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":512}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-23 16:58:59,572 - INFO - flashinfer.jit: Prebuilt kernels not found, using JIT backend\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING 05-23 16:58:59 [utils.py:2522] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,initialize_cache not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7fa3f025b1d0>\n",
      "INFO 05-23 16:59:00 [parallel_state.py:1004] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0\n",
      "INFO 05-23 16:59:00 [cuda.py:221] Using Flash Attention backend on V1 engine.\n",
      "INFO 05-23 16:59:00 [topk_topp_sampler.py:44] Currently, FlashInfer top-p & top-k sampling sampler is disabled because FlashInfer>=v0.2.3 is not backward compatible. Falling back to the PyTorch-native implementation of top-p & top-k sampling.\n",
      "INFO 05-23 16:59:00 [gpu_model_runner.py:1329] Starting to load model TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ...\n",
      "INFO 05-23 16:59:00 [weight_utils.py:265] Using model weights format ['*.safetensors']\n",
      "INFO 05-23 16:59:02 [weight_utils.py:281] Time spent downloading weights for TheBloke/TinyLlama-1.1B-Chat-v0.3-GPTQ: 2.162578 seconds\n",
      "INFO 05-23 16:59:02 [weight_utils.py:315] No model.safetensors.index.json found in remote.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.91it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:00<00:00,  5.91it/s]\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-23 16:59:02 [loader.py:458] Loading weights took 0.20 seconds\n",
      "INFO 05-23 16:59:02 [punica_selector.py:18] Using PunicaWrapperGPU.\n",
      "INFO 05-23 16:59:03 [gpu_model_runner.py:1347] Model loading took 1.1657 GiB and 2.789998 seconds\n",
      "INFO 05-23 16:59:11 [backends.py:420] Using cache directory: /home/ubuntu/.cache/vllm/torch_compile_cache/e57ad72506/rank_0_0 for vLLM's torch.compile\n",
      "INFO 05-23 16:59:11 [backends.py:430] Dynamo bytecode transform time: 7.63 s\n",
      "INFO 05-23 16:59:14 [backends.py:136] Cache the graph of shape None for later use\n",
      "INFO 05-23 16:59:43 [backends.py:148] Compiling a graph for general shape takes 32.23 s\n",
      "INFO 05-23 17:00:01 [monitor.py:33] torch.compile takes 39.86 s in total\n",
      "INFO 05-23 17:00:01 [kv_cache_utils.py:634] GPU KV cache size: 1,790,608 tokens\n",
      "INFO 05-23 17:00:01 [kv_cache_utils.py:637] Maximum concurrency for 2,048 tokens per request: 874.32x\n",
      "INFO 05-23 17:00:49 [gpu_model_runner.py:1686] Graph capturing finished in 47 secs, took 2.59 GiB\n",
      "INFO 05-23 17:00:49 [core.py:159] init engine (profile, create kv cache, warmup model) took 106.18 seconds\n",
      "INFO 05-23 17:00:49 [core_client.py:439] Core engine process 0 ready.\n",
      "ERROR 05-23 17:00:49 [core.py:398] EngineCore encountered a fatal error.\n",
      "ERROR 05-23 17:00:49 [core.py:398] Traceback (most recent call last):\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\n",
      "ERROR 05-23 17:00:49 [core.py:398]     engine_core.run_busy_loop()\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\n",
      "ERROR 05-23 17:00:49 [core.py:398]     self._process_engine_step()\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\n",
      "ERROR 05-23 17:00:49 [core.py:398]     outputs = self.step_fn()\n",
      "ERROR 05-23 17:00:49 [core.py:398]               ^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 203, in step\n",
      "ERROR 05-23 17:00:49 [core.py:398]     output = self.model_executor.execute_model(scheduler_output)\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n",
      "ERROR 05-23 17:00:49 [core.py:398]     output = self.collective_rpc(\"execute_model\",\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "ERROR 05-23 17:00:49 [core.py:398]     answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return func(*args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return func(*args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 268, in execute_model\n",
      "ERROR 05-23 17:00:49 [core.py:398]     output = self.model_runner.execute_model(scheduler_output)\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return func(*args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1173, in execute_model\n",
      "ERROR 05-23 17:00:49 [core.py:398]     prompt_logprobs_dict = self._get_prompt_logprobs_dict(\n",
      "ERROR 05-23 17:00:49 [core.py:398]                            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1413, in _get_prompt_logprobs_dict\n",
      "ERROR 05-23 17:00:49 [core.py:398]     logits = self.model.compute_logits(prompt_hidden_states, None)\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 565, in compute_logits\n",
      "ERROR 05-23 17:00:49 [core.py:398]     logits = self.logits_processor(self.lm_head, hidden_states,\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return self._call_impl(*args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return forward_call(*args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/lora/layers.py\", line 1164, in forward\n",
      "ERROR 05-23 17:00:49 [core.py:398]     return type(self.base_layer).forward(self, *args, **kwargs)\n",
      "ERROR 05-23 17:00:49 [core.py:398]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 70, in forward\n",
      "ERROR 05-23 17:00:49 [core.py:398]     logits = self._get_logits(hidden_states, lm_head, embedding_bias)\n",
      "ERROR 05-23 17:00:49 [core.py:398]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ERROR 05-23 17:00:49 [core.py:398]   File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/lora/layers.py\", line 1150, in _get_logits\n",
      "ERROR 05-23 17:00:49 [core.py:398]     logits[:,\n",
      "ERROR 05-23 17:00:49 [core.py:398] RuntimeError: The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 0.  Target sizes: [3, 256].  Tensor sizes: [4, 256]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process EngineCore_0:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 400, in run_engine_core\n",
      "    raise e\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 389, in run_engine_core\n",
      "    engine_core.run_busy_loop()\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 413, in run_busy_loop\n",
      "    self._process_engine_step()\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 438, in _process_engine_step\n",
      "    outputs = self.step_fn()\n",
      "              ^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core.py\", line 203, in step\n",
      "    output = self.model_executor.execute_model(scheduler_output)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/executor/abstract.py\", line 86, in execute_model\n",
      "    output = self.collective_rpc(\"execute_model\",\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/executor/uniproc_executor.py\", line 56, in collective_rpc\n",
      "    answer = run_method(self.driver_worker, method, args, kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/utils.py\", line 2456, in run_method\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_worker.py\", line 268, in execute_model\n",
      "    output = self.model_runner.execute_model(scheduler_output)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1173, in execute_model\n",
      "    prompt_logprobs_dict = self._get_prompt_logprobs_dict(\n",
      "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/worker/gpu_model_runner.py\", line 1413, in _get_prompt_logprobs_dict\n",
      "    logits = self.model.compute_logits(prompt_hidden_states, None)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/model_executor/models/llama.py\", line 565, in compute_logits\n",
      "    logits = self.logits_processor(self.lm_head, hidden_states,\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1739, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1750, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/lora/layers.py\", line 1164, in forward\n",
      "    return type(self.base_layer).forward(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/model_executor/layers/logits_processor.py\", line 70, in forward\n",
      "    logits = self._get_logits(hidden_states, lm_head, embedding_bias)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/lora/layers.py\", line 1150, in _get_logits\n",
      "    logits[:,\n",
      "RuntimeError: The expanded size of the tensor (3) must match the existing size (4) at non-singleton dimension 0.  Target sizes: [3, 256].  Tensor sizes: [4, 256]\n"
     ]
    },
    {
     "ename": "EngineDeadError",
     "evalue": "EngineCore encountered an issue. See stack trace (above) for the root cause.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mEngineDeadError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      8\u001b[39m lora_path = snapshot_download(repo_id=test_config[\u001b[33m'\u001b[39m\u001b[33mlora_repo\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      9\u001b[39m test_prompts = create_test_prompts(lora_path)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[43mprocess_requests\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_prompts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# Clean up the GPU memory for the next test\u001b[39;00m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m engine\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mprocess_requests\u001b[39m\u001b[34m(engine, test_prompts)\u001b[39m\n\u001b[32m     42\u001b[39m     engine.add_request(\u001b[38;5;28mstr\u001b[39m(request_id),\n\u001b[32m     43\u001b[39m                        prompt,\n\u001b[32m     44\u001b[39m                        sampling_params,\n\u001b[32m     45\u001b[39m                        lora_request=lora_request)\n\u001b[32m     46\u001b[39m     request_id += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m request_outputs: \u001b[38;5;28mlist\u001b[39m[RequestOutput] = \u001b[43mengine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     49\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m request_output \u001b[38;5;129;01min\u001b[39;00m request_outputs:\n\u001b[32m     50\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m request_output.finished:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/llm_engine.py:218\u001b[39m, in \u001b[36mLLMEngine.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m []\n\u001b[32m    217\u001b[39m \u001b[38;5;66;03m# 1) Get EngineCoreOutput from the EngineCore.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m218\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine_core\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# 2) Process EngineCoreOutputs.\u001b[39;00m\n\u001b[32m    221\u001b[39m processed_outputs = \u001b[38;5;28mself\u001b[39m.output_processor.process_outputs(\n\u001b[32m    222\u001b[39m     outputs.outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/conda/envs/grok/lib/python3.12/site-packages/vllm/v1/engine/core_client.py:558\u001b[39m, in \u001b[36mSyncMPClient.get_output\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    556\u001b[39m outputs = \u001b[38;5;28mself\u001b[39m.outputs_queue.get()\n\u001b[32m    557\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(outputs, \u001b[38;5;167;01mException\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m558\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_exception(outputs) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    559\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "\u001b[31mEngineDeadError\u001b[39m: EngineCore encountered an issue. See stack trace (above) for the root cause."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:[W523 17:00:50.599917580 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n"
     ]
    }
   ],
   "source": [
    "for test_config in test_configs:\n",
    "    print(\n",
    "        f\"~~~~~~~~~~~~~~~~ Running: {test_config['name']} ~~~~~~~~~~~~~~~~\"\n",
    "    )\n",
    "    engine = initialize_engine(test_config['model'],\n",
    "                                test_config['quantization'],\n",
    "                                test_config['lora_repo'])\n",
    "    lora_path = snapshot_download(repo_id=test_config['lora_repo'])\n",
    "    test_prompts = create_test_prompts(lora_path)\n",
    "    process_requests(engine, test_prompts)\n",
    "\n",
    "    # Clean up the GPU memory for the next test\n",
    "    del engine\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c056265f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba361d41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
